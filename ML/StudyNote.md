# ML(机器学习)学习笔记
****
## 一、机器学习简介
全卷积网络（Fully Convolutional Networks，FCN）是Jonathan Long等人于2015年在Fully Convolutional Networks for Seman  tic Segmentation一文中提出的用于图像语义分割的一种框架，是深度学习用于语义分割领域的开山之作。FCN将传统CNN后面的全连接层换成了卷积层，这样网络的输出将是热力图而非类别；同时，为解决卷积和池化导致图像尺寸的变小，使用上采样方式对图像尺寸进行恢复。
![ML 框架图](https://948021660-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LYtTdBhaX9EsGjMVkHV%2F-LYzhzSh55h1KzjtX1qG%2F-LYziGoVyFXqVWqqtZsI%2Fimage.png?alt=media&token=9ff95774-9f9e-44e6-93df-b1da30797345)


- **人工智能**(Artificial Intelligence)就是用机器模拟人的意识和思维。
- **机器学习**(Machine Learning)则是实现人工智能的一种方法，是人工智能的子集。
- **深度学习**(Deep Learning)就是深层次神经网络，是机器学习的一种实现方法，是机器学习的子集。

在20世纪50年代中期，人工智能开始兴起。到了20世纪80年代，机器学习开始繁荣起来，在这张图中的垃圾邮件分类就属于机器学习。到2010年左右，深度学习得到了极大的发展，把人工智能推向了新的高潮，最常见的深度学习有图片分类等等。
机器学习是人工智能的一个分支，它是实现人工智能的一个核心技术，即以机器学习为手段解决人工智能中的问题。
机器学习是通过一些让计算机可以自动“学习”的算法并从数据中分析获得规律，然后利用规律对新样本进行预测。
机器学习如果用形式化的语言进行描述，就是对于某类任务T和性能度量P，如果一个计算机程序在T上以P衡量的性能随着经验E而自我完善，那么就称这个计算机程序在从经验E学习。
人类在学习中是什么样的呢？我们可以举一个简单的例子：一个小宝宝，他的妈妈买回来一个苹果并告诉他这是苹果，那么他就会对苹果有所认知，第二天，他的妈妈买了一个不同样子的苹果，但是告诉他这个还是苹果，那么他就会对苹果有新的认知，经过认识不同种类的苹果，小宝宝对苹果形成了自己的认知，可以去判断什么样的东西是苹果。他能根据自己的经验总结出一个规律，然后对于新看到的物品可以去判断它是否是苹果，其他的水果也是同理。这样就完成了一个人类学习的过程。
****
## 二、经典算法
1. 决策树(Decision Tree)
2. 支持向量机(Support Vector Machine)
3. 集成算法
    a. 装袋算法(Bagging)
    b. 提升算法(Boosting)
    c. 堆叠算法(Stacking)
    d. .......
4. ......
****
## 三、决策树解析
### 3.1、决策树
#### 3.1.1、基本概念
决策树：是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果，本质是一颗由多个判断节点组成的树。
![image.png](https://img-blog.csdnimg.cn/20201012103411570.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hc3Rlcl9odW50ZXI=,size_16,color_FFFFFF,t_70)
#### 3.1.2、常用术语
**结点(节点)node**：样本数据的一个集合。 包括**根节点roof**、**内部节点**、**叶子节点leaf**

**分枝split**：依据什么原则将一个节点分割。

**剪枝** **预剪枝**：在创建树的同时进行剪枝 **后剪枝**：树建好了再修建
#### 3.1.3、树模型的优势
**自动处理大量自变量** 树模型会在所有自变量中依次选出最重要的自变量进行对样本进行切分。(自变量可以重复使用)

**对数据没有正态独立方差齐这些要求** 应用范围更广，
### 3.2、树的构造
所谓"构造"，就是**选择一个自变量，依据某个标准，来分割节点**。
#### 信息熵
- 信息熵表示的是信息的不确定程度。信息越不确定，熵值越大。
#### 信息增益（ID3 算法）、信息增益率（C4.5 算法）、基尼指数（Cart 算法）
- **信息增益**指的就是某个划分可以带来纯度的提高，信息熵的下降。

- 依据信息增益构建树时，没有考虑自变量数量(属性或特征)的多少，ID3算法倾向于选择取值比较多的自变量(属性)。

- **信息增益率**=信息增益/属性熵。C4.5算法

- cart算法，Classification and Regression Tree，分类回归树。

- ID3与C4.5可以生成二叉树和多叉数，但**cart只能二叉树，既可分类树，也可以做回归树。**

#### 基尼系数
- p(C<sub>k</sub>|t) 表示节点t属于类别 C<sub>k</sub> 的概率，节点 t 的基尼系数为 1 减去各类别 C<sub>k</sub> 概率平方和。

- 基尼系数越小，说明样本之间的差异性小，不确定程度低。

- 把一个节点分为两个后，算它们归一化基尼系数和

- **从所有的可能划分中找出Gini系数最小(Gini系数改变最大)的划分**

#### 算法原理
分类解决离散问题，回归解决连续问题。
1. 分类树是基于概率来构建的,采用信息增益、信息增益率、基尼系数来作为树的评价指标。
2. 回归数是基于平均值来构建的,采用均方差作为树的评价指标。决策树：信息论；逻辑回归、贝叶斯：概率论。不同于逻辑斯蒂回归和贝叶斯算法，决策树的构造过程不依赖领域知识，它使用属性选择度量来选择将元组最好地划分成不同的类的属性。所谓决策树的构造就是进行属性选择度量确定各个特征属性之间的拓扑结构。决策树对空数据，异常值都不敏感，任何类型的数据都支持，不需要特征处理，不用做特征工程。

构造决策树的关键步骤是分裂属性。所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。分裂属性分为三种不同的情况：

1. 属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。

2. 属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。

3. 属性是连续值。此时确定一个值作为分裂点split_point，按照>split_point和<=split_point生成两个分支。

构造决策树的关键性内容是进行属性选择度量，属性选择度量是一种选择分裂准则，它决定了拓扑结构及分裂点split_point的选择。属性选择度量算法有很多，如ID3，C4.5等，一般使用自顶向下递归分治法，并采用不回溯的贪心策略（只考虑当前数据特征的最好分割方式,不能回溯操作，只能从上往下分割)。

#### 决策树构建过程（步骤）：

1. 将所有的特征看成一个一个的节点；

2. 遍历所有特征,遍历到其中某一个特征时:遍历当前特征的所有分割方式，找到最好的分割点,将数据划分为不同的子节点,计算划分后子节点的纯度信息；

3. 在遍历的所有特征中,比较寻找最优的特征以及最优特征的最优划分方式,纯度越高,则对当前数据集进行分割操作；

4. 对新的子节点继续执行2-3步,直到每个最终的子节点都足够纯。

#### 决策树算法构建的停止条件：

1. 当子节点中只有一种类型的时候停止构建(会导致过拟合)

2. 当前节点种样本数小于某个值，同时迭代次数达到指定值，停止构建，此时使用该节点中出现最多的类别样本数据作为对应值(比较常用)

### 3.3、树的剪枝

#### 3.3.1、为什么要剪枝
随着树的增长， 在训练样集上的精度是单调上升的，然而在独立的测试样例上测出的精度先上升后下降。
原因1： 噪声、 样本冲突， 即错误的样本数据。
原因2： 特征即属性不能完全作为分类标准。
原因3： 巧合的规律性， 数据量不够大。
#### 3.3.2、常用的剪枝方法
（1）预剪枝:
    a. 每一个结点所包含的最小样本数目，例如10，则该结点总样本数小于10时，则不
    再分；
    b. 指定树的高度或者深度，例如树的最大深度为4；
    c. 指定结点的熵小于某个值，不再划分。随着树的增长， 在训练样集上的精度是调上升的， 然而在独立的测试样例上测出的精度先上升后下降。

（2）后剪枝：
后剪枝，在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。
主要有四种：
    a. REP-错误率降低剪枝
    b. PEP-悲观剪枝
    c. CCP-代价复杂度剪枝
    d. MEP-最小错误剪枝
#### 3.3.3、结论：
决策树的剪枝，由于生成的决策树存在过拟合问题，需要对它进行剪枝，以简化学到的决策树。决策树的剪枝，往往从已生成的树上剪掉一些叶节点或叶节点以上的子树，并将其父节点或根节点作为新的叶节点，从而简化生成的决策树模型。

## 四、支持向量机解析
SVM是什么? 先来看看维基百科上对SVM的定义:

>支持向量机（英语：support vector machine，常简称为SVM，又名支持向量网络）是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法创建一个将新的实例分配给两个类别之一的模型，使其成为非概率二元线性分类器。SVM模型是将实例表示为空间中的点，这样映射就使得单独类别的实例被尽可能宽的明显的间隔分开。然后，将新的实例映射到同一空间，并基于它们落在间隔的哪一侧来预测所属类别。

如果从未接触SVM的话，维基的这一大段解释肯定会让你一头雾水。简单点讲，SVM就是一种二类分类模型，他的基本模型是的定义在特征空间上的间隔最大的线性分类器，SVM的学习策略就是间隔最大化。
![SVM 直观理解](https://pic3.zhimg.com/80/v2-0d6eb1459f72940dc932795f345bae06_720w.webp)
图中有分别属于两类的一些二维数据点和三条直线。如果三条直线分别代表三个分类器的话，请问哪一个分类器比较好？

我们凭直观感受应该觉得答案是H3。首先H1不能把类别分开，这个分类器肯定是不行的；H2可以，但分割线与最近的数据点只有很小的间隔，如果测试数据有一些噪声的话可能就会被H2错误分类(即对噪声敏感、泛化能力弱)。H3以较大间隔将它们分开，这样就能容忍测试数据的一些噪声而正确分类，是一个泛化能力不错的分类器。

对于支持向量机来说，数据点若是维向量，我们用 P-1 维的超平面来分开这些点。但是可能有许多超平面可以把数据分类。最佳超平面的一个合理选择就是以最大间隔把两个类分开的超平面。因此，SVM选择能够使离超平面最近的数据点的到超平面距离最大的超平面。

详细的数学推导在CSDN博客上：

>http://blog.csdn.net/zouxy09/article/details/17291543
>http://blog.csdn.net/v_july_v/article/details/7624837

**SVM总结**

任何算法都有其优缺点，支持向量机也不例外。

支持向量机的**优点**是:
由于SVM是一个凸优化问题，所以求得的解一定是全局最优而不是局部最优。
不仅适用于线性线性问题还适用于非线性问题(用核技巧)。
拥有高维样本空间的数据也能用SVM，这是因为数据集的复杂度只取决于支持向量而不是数据集的维度，这在某种意义上避免了“维数灾难”。
理论基础比较完善(例如神经网络就更像一个黑盒子)。

支持向量机的**缺点**是:
二次规划问题求解将涉及m阶矩阵的计算(m为样本的个数), 因此SVM不适用于超大数据集。(SMO算法可以缓解这个问题)
只适用于二分类问题。(SVM的推广SVR也适用于回归问题；可以通过多个SVM的组合来解决多分类问题)

## 五、集成算法
集成算法是构建多个学习器，通过一定策略结合来完成学习任务。正所谓三个臭皮匠顶一个诸葛亮，当弱学习器被正确组合时，我们能得到更精确、鲁棒性更好的学习器。由于个体学习器在准确性和多样性存在冲突，追求多样性势必要牺牲准确性。这就需要将这些“好而不同”的个体学习器结合起来。而研究如何产生并结合个体学习器也是集成学习研究的核心。

**集成学习的思想时将这些弱学习器的偏置或方差结合起来，从而创建一个强学习机，获得更好的性能**

按照个体学习器之间的关系，集成算法分为**Bagging、Boosting、Stacking**三大类。

**Bagging**
基于自主采样法（bootstrap sampling）随即得到一些样本集训练，用来分别训练不同的基学习器，然后对不同的基学习器得到的结果投票得出最终的分类结果。自主采样法得到的样本大概会有63%的数据样本被使用(有些数据样本会被重复抽取，有些不会被抽取），剩下的可以用来做验证集。
![Bagging 结构图](https://pic1.zhimg.com/v2-e29a9143b745d9bd1095a58f46a71648_r.jpg)


Bagging中各个算法之间没有依赖，可以并行计算，他的结果参考了各种情况，实现的是在欠拟合和过拟合之间取折中。

**Boosting**
Boosting，提升算法，通过反复学习得到一系列弱分类器，然后组合这些弱分类器得到一个强分类器，吧弱学习器提升为强学习器的过程。主要分为两个部分，加法模型和向前分布。

加法模型就是把一系列弱学习器相加串联为强学习器，


其中，  是一些列的弱学习器，  是该学习器训练得到的最优参数，  是对应弱学习器在强学习器中所占比例的系数。

向前分布是指本轮中的学习器是在上一轮学习器的基础上迭代训练得到的，

![Boosting 结构图](https://pic4.zhimg.com/80/v2-3ab9af71ed255ed147730f25d924f67f_720w.webp)


训练过程为阶梯状，基模型按照次序一一进行训练，基模型的训练集按照某种策略每次都进行一定的转化，如果某一个数据在这次分错了，那么在下一次就会给他更大的权重。对所有基模型预测的结果进行线性综合产生最终的预测结果。

一般来说，他的效果会比Bagging好一些，由于新模型是在就模型的基本上建立的，因此不能使用并行方式训练，并且由于对错误样本的关注，也可能造成过拟合。

**stacking**
stacking训练一个模型用于组合其他各个基模型。具体方法是吧数据分成两部分，用其中一部分训练几个基模型1、2、3，用另一部分数据测试这几个基模型，把1、2、3的输出作为输入，训练组合模型。注意，他不是把模型的结果组织起来，而把模型组织起来。理论上，stacking可以组织任何模型，实际中常用单层logistic回归作为模型。
![Boosting 结构图](https://pic2.zhimg.com/80/v2-d6fd0083aba437c04f9e5e98081afe25_720w.webp)


**举个栗子**：

![Boosting 结构图](https://pic2.zhimg.com/v2-4f5b3af0942557a03fd2871df95d1711_r.jpg)

1. 首先我们会得到两组数据：训练集和测试集。将训练集分成5份：train1,train2,train3,train4,train5。
2. 选定基模型。这里假定我们选择了xgboost, lightgbm 和 randomforest这三种作为基模型。比如xgboost模型部分：依次用train1,train2,train3,train4,train5作为验证集，其余4份作为训练集，进行5折交叉验证进行模型训练；再在测试集上进行预测。这样会得到在训练集上由xgboost模型训练出来的5份predictions，和在测试集上的1份预测值B1。将这五份纵向重叠合并起来得到A1。lightgbm和randomforest模型部分同理。
3. 三个基模型训练完毕后，将三个模型在训练集上的预测值作为分别作为3个"特征"A1,A2,A3，使用LR模型进行训练，建立LR模型。
4. 使用训练好的LR模型，在三个基模型之前在测试集上的预测值所构建的三个"特征"的值(B1,B2,B3)上，进行预测，得出最终的预测类别或概率。
5. stacking一般是两层就够了。